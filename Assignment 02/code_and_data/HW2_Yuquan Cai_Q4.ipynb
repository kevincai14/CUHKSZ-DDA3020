{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 4: Convolutional Neural Network\n",
    "**Course Name:** Machine Learning (DDA3020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>*Please enter your personal information (Double-click this block first)*</font>\n",
    "\n",
    "**Name:** CAI, Yuquan\n",
    "\n",
    "**Student ID:** 122090007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's highly recommended to finish Question 3 first.**\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this question, you will implement two CNN models and train them on the same dataset as Question 3 (Fasion-MNIST). We will discover how well-suited CNNs are for intensive data tasks such as image processing, compared to traditional machine learning algorithms (like those tree tree-based models in Question 3). Similarly, your task is to **run all codes in this script and complete the parts marked with** <font color=Red>\\[TASK\\]</font>.\n",
    "\n",
    "### Introduction of TensorFlow\n",
    "\n",
    "TensorFlow is a powerful open-source package for machine learning and deep learning, enabling efficient implementation of complex models like neural networks with ease. First of all, you need to install the TensorFlow package with the version of 2.9\n",
    "\n",
    "```bash\n",
    "pip install numpy==1.26 tensorflow==2.9 -i https://mirrors.aliyun.com/pypi/simple/ \n",
    "```\n",
    "\n",
    "by running this commend in your command line window. To check whether the package is successfully installed, you can try to run the following import block. If you're struggling with installing TensorFlow, you can upload the coding assignment to Google Colab for execution or seek assistance from TA."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:02:07.791157Z",
     "start_time": "2025-04-02T19:02:05.162651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to carefully read this block since it's just loading the dataset. Just run it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:02:11.274384Z",
     "start_time": "2025-04-02T19:02:11.254382Z"
    }
   },
   "source": [
    "def load_mnist(path, kind, subset=None):\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz'%kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'%kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "    \n",
    "    if subset is not None:\n",
    "        selected_images, selected_labels = [], []\n",
    "        for label in range(10):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            selected_indices = np.random.choice(indices, subset, replace=False)\n",
    "            selected_images.append(images[selected_indices])\n",
    "            selected_labels.append(labels[selected_indices])\n",
    "        images = np.concatenate(selected_images, axis=0)\n",
    "        labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "        paired = list(zip(images, labels))\n",
    "        random.shuffle(paired)\n",
    "        images, labels = zip(*paired)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will use all data of Fashion-MNIST and do a little bit data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:02:14.580958Z",
     "start_time": "2025-04-02T19:02:14.196142Z"
    }
   },
   "source": [
    "X_train, y_train = load_mnist('./data/', kind='train')\n",
    "X_test, y_test = load_mnist('./data/', kind='t10k')\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with TensorFlow or PyTorch, you can straightly start Task 1. Otherwise, it's highly recommended to start from https://www.tensorflow.org/tutorials/images/cnn to gain some basic capacity on building neural network by TensorFlow.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "At the beginning, we need to build a very simple CNN model with the structure of\n",
    "1. A 2D convolutional layer with 16 filters with each size 3*3 (RELU activation function)\n",
    "2. A 2D maxpooling layer with 2*2 pooling window\n",
    "3. A flatten layer to convert 2D feature into 1D vector\n",
    "4. A fully connected layer using Softmax activation\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. <font color=Red>\\[TASK\\]</font> (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:07:26.604480Z",
     "start_time": "2025-04-02T19:07:15.473791Z"
    }
   },
   "source": [
    "##########################\n",
    "## Write your code here ##\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "##########################\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1)  # Train the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)                        #  Test the model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4784 - accuracy: 0.8351 - val_loss: 0.3718 - val_accuracy: 0.8677\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3325 - accuracy: 0.8856 - val_loss: 0.3157 - val_accuracy: 0.8892\n",
      "313/313 [==============================] - 0s 984us/step - loss: 0.3252 - accuracy: 0.8861\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Then we will take a challenge to implement a more complex CNN model to have a better classification performance. Here is a structure for your reference, or you can also design your own CNN model. The only requirement is to have a better performance than the simple CNN in Task 1 (a larger accuracy score on test set).\n",
    "\n",
    "The reference structure is devided into three parts:\n",
    "1. Primary Feature Extraction Part\n",
    "    1. A 2D convolutional layer with 32 filters with each size 3*3 (RELU activation function)\n",
    "    2. A normalization layer\n",
    "    3. A 2D maxpooling layer with 2*2 pooling window\n",
    "    4. A dropout layer (randomly drops 25% of units) (designed for preventing overfitting)\n",
    "2. Advanced Feature Extraction Part\n",
    "\n",
    "    This part is mostly similar to the previous section. The only difference is to use more filters (like 64) in convolutional layer to gain high-level features\n",
    "3. Classification Part\n",
    "    1. A flatten layer to convert 2D feature into 1D vector\n",
    "    2. A fully connected layer with 512 units using RELU to summerize high-dimensinal features\n",
    "    3. Another connected layer for Softmax classification\n",
    "\n",
    "Remember that we are doing a image classification task, so we shall use categorical cross entropy function as the loss funtion. <font color=Red>\\[TASK\\]</font> (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T19:13:50.842479Z",
     "start_time": "2025-04-02T19:13:25.040435Z"
    }
   },
   "source": [
    "##########################\n",
    "## Write your code here ##\n",
    "model = models.Sequential([\n",
    "    # 1. Primary Feature Extraction Part\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # 2. Advanced Feature Extraction Part\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # 3. Classification Part\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "##########################\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.1)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.4493 - accuracy: 0.8410 - val_loss: 0.3219 - val_accuracy: 0.8843\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.3120 - accuracy: 0.8844 - val_loss: 0.3027 - val_accuracy: 0.8880\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3180 - accuracy: 0.8793\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
